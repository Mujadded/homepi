version: '3.8'

services:
  inference:
    image: nvcr.io/nvidia/pytorch:24.09-py3
    container_name: homepi-inference
    runtime: nvidia
    ports:
      - "5001:5001"
    volumes:
      - ./jetson_inference_server.py:/app/inference_server.py:ro
      - inference-cache:/root/.cache
    working_dir: /app
    command: python3 inference_server.py
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  inference-cache:

